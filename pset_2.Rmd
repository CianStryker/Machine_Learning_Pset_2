---
title: 'Problem Set #2'
author: "Cian Stryker"
date: "10/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(AER)
library(tidyverse)
library(fastDummies)
```

# Conceptual Questions


#### Question 1

\hfill\break

a.)

```{r}
Obs_1 = (0-1)^2 + (2-1)^2 + (2-1)^2
Obs_2 = (2-1)^2 + (1-1)^2 + (2-1)^2  
Obs_3 = (0-1)^2 + (0-1)^2 + (0-1)^2 
Obs_4 = (0-1)^2 + (1-1)^2 + (1-1)^2
Obs_5 = (0-1)^2 + (1-1)^2 + (2-1)^2 
Obs_6 = (1-1)^2 + (1-1)^2 + (3-1)^2 

dist_1 = sqrt(Obs_1)
dist_2 = sqrt(Obs_2)
dist_3 = sqrt(Obs_3)
dist_4 = sqrt(Obs_4)
dist_5 = sqrt(Obs_5)
dist_6 = sqrt(Obs_6)

d = (3 + 0.5 + 2.5)/3
```

The Euclidean distance between observation 3 and the test point is 1.732. 

b.)

Our prediction would be Black because the closest observation to the test point is observation 4, which is black. 

c.)

The three closest observations to the test point are observations 2, 4, and 5. Since observation 2 and 5 are white, our prediction with K = 3 would also be white. The majority of the closest three points are white. 

d.)

The prediction would be Y = 2 because the average of the three closest observations (2, 4, and 5) is 2. 

\hfill\break


#### Question 2

\hfill\break

```{r}
question2 <- 2^30
```

You would have 1,073,741,824 combinations with 30 potential covariates. This indicates that you would be better off using forward/backward selection instead of best subset selection. Trying to go with best subset selection would require you to try all 1 billion or so possible combinations, which would obviously take a ton of time. By using forward/backward selection you will save a lot of time and likely come very close to the accuracy of best subset selection, but it will be slightly worse. 

#### Question 3

\hfill\break


#### Question 4

\hfill\break

I believe that I should side with Colleage A because KNN is non-parametric and will be able to work with the data that we have. Since we don't know anything about the error distribution or whether the relationship is linear, we should avoid linear regression. I think that KNN would be a good approach to this situation. 

#### Question 5

\hfill\break

Colleague C may be concerned because QDA assumes that the measurments from each class are normally distributed, but with such a large dataset, that may not be the case. If it is not normally distributed then QDA would not be appropriate to use. 

#### Question 6

\hfill\break

I would expect that logistic regression would outperform LDA because all our predictors are categorical. 

#### Question 7

\hfill\break



#### Question 8

\hfill\break



#### Question 9

\hfill\break



\hfill\break


# Data Questions


```{r Setting up and Question 1}

data("Fatalities")

rows <- Fatalities %>%
  nrow()

rows_no_nas <- Fatalities %>%
  drop_na() %>%
  nrow()

data <- Fatalities %>%
  drop_na()

```

1.) One observation has missing values for at least one feature. 

```{r Question 2}

 q3 <- sapply(data, class)

```

2.) There are four categorical variables in the data set: state, breath, jail, and service. State has 50 different classes, but breath, jail, and service each only have two classes. 

```{r Question 3}

data2 <- data %>%
  dummy_cols(select_columns = c("state", "breath", "jail", "service"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)

model_1 <- lm(data = data2, fatal ~.)

summary_1 <- summary(model_1)

```

3.) The adjusted R^2 value is 0.999

```{r Question 4}

```

